{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Dan Constantini, Tom Hayat et Alexandre Attia\n",
    "This script loads 11 data class from imagenet and sort the files according to their name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- TRAIN SET\n",
    "require 'torch'\n",
    "require 'xlua'\n",
    "require 'image'\n",
    "require 'nn'\n",
    "require 'optim'\n",
    "\n",
    "classes={'bridge', 'building', 'city', 'eiffel_tower','elephant', 'landscape', 'lion', 'monkey', 'people', 'tower', 'water'}\n",
    "\n",
    "imagesAll = torch.Tensor(1721,3,64,64) \n",
    "labelsAll = torch.Tensor(1721)\n",
    "compteur = 0 -- compteur pour label selon les dir\n",
    "compteur2 = 0 --compteur pour les images N&B\n",
    "\n",
    "for dir in paths.iterdirs(\"./dataset/train/\") do\n",
    "    compteur = compteur +1\n",
    "    files = {}\n",
    "    for file in paths.files(paths.concat('./dataset/train/', dir)) do\n",
    "       if file:find('JPEG' .. '$') then\n",
    "          table.insert(files, paths.concat(paths.concat('./dataset/train/',dir), file))\n",
    "       end\n",
    "    end\n",
    "   \n",
    "    if #files == 0 then\n",
    "       error('given directory doesnt contain any files of type: ')\n",
    "    end\n",
    "\n",
    "    -- print(#files)\n",
    "    temp2 = compteur2\n",
    "    temp8=0\n",
    "     for i=1,(#files) do\n",
    "        temp = image.load(files[i])\n",
    "\n",
    "        if (temp:size()[1]==3) then --on ne prend en compte que les images couleurs\n",
    "            compteur2 = compteur2 + 1\n",
    "            imagesAll[temp2 + i] = image.load(files[i]) \n",
    "            labelsAll[temp2 + i] = compteur\n",
    "        end\n",
    "        if (temp:size()[1]==1) then\n",
    "            --à ajouter : la transformation d'une N&B en RGB\n",
    "            temp8=temp8+1\n",
    "        end\n",
    "    end\n",
    "    collectgarbage()\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- TEST SET\n",
    "imagesTestAll = torch.Tensor(331,3,64,64) \n",
    "labelsTestAll = torch.Tensor(331)\n",
    "compteur3 = 0 -- compteur pour label selon les dir\n",
    "compteur4 = 0 --compteur pour les images N&B\n",
    "\n",
    "for dir in paths.iterdirs(\"./dataset/val/\") do\n",
    "    compteur3 = compteur3 +1\n",
    "    filesTest = {}\n",
    "    for file in paths.files(paths.concat('./dataset/val/', dir)) do\n",
    "       if file:find('JPEG' .. '$') then\n",
    "          table.insert(filesTest, paths.concat(paths.concat('./dataset/val/',dir), file))\n",
    "           \n",
    "       end\n",
    "    end\n",
    "   \n",
    "    if #filesTest == 0 then\n",
    "       error('given directory doesnt contain any files of type: ')\n",
    "    end\n",
    "\n",
    "    -- print(#files)\n",
    "    temp3 = compteur4\n",
    "     for i=1,(#filesTest) do\n",
    "        temp4 = image.load(filesTest[i])\n",
    "\n",
    "        if (temp4:size()[1]==3) then --on ne prend en compte que les images couleurs\n",
    "            compteur4 = compteur4 + 1\n",
    "            imagesTestAll[temp3 + i] = image.load(filesTest[i]) \n",
    "            labelsTestAll[temp3 + i] = compteur2\n",
    "        end\n",
    "    end\n",
    "    collectgarbage()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Nombre d'images couleurs: 1683 \n",
    "-- Creation du dataset\n",
    "trainData = {\n",
    "   data = torch.Tensor(1685, 3, 64, 64),\n",
    "   labels = torch.Tensor(1685),\n",
    "}\n",
    "\n",
    "-- Nombre d'images couleurs : 325\n",
    "-- Creation test set:\n",
    "testData = {\n",
    "      data = torch.Tensor(325, 3, 64, 64),\n",
    "      labels = torch.Tensor(325),\n",
    "   }\n",
    "\n",
    "for i=1,1685 do\n",
    "   trainData.data[i] = imagesAll[i]\n",
    "   trainData.labels[i] = labelsAll[i]\n",
    "end\n",
    "for i=1,325 do\n",
    "   testData.data[i] = imagesTestAll[i]\n",
    "   testData.labels[i] = labelsTestAll[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  data : DoubleTensor - size: 1685x3x64x64\n",
       "  labels : DoubleTensor - size: 1685\n",
       "}\n",
       "{\n",
       "  data : DoubleTensor - size: 325x3x64x64\n",
       "  labels : DoubleTensor - size: 325\n",
       "}\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainData)\n",
    "print(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAAIklEQVRoge3BAQ0AAADCoPdPbQ8HFAAAAAAAAAAAAAAA8G4wQAAB9qDgAAAAAABJRU5ErkJggg==",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 64,
       "width": 64
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.7363578108483e-318\t\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itorch.image(trainData.data[42])\n",
    "print(trainData.labels[42])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On convertit nos images RGB en YUV (par rapport à la luminance) puis on normalize (retranche la moyenne et la deviation). Normalisation locale et globale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: colorspace RGB -> YUV\t\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Convert all images to YUV\n",
    "print '==> preprocessing data: colorspace RGB -> YUV'\n",
    "for i = 1,1685 do\n",
    "   trainData.data[i] = image.rgb2yuv(trainData.data[i])\n",
    "end\n",
    "for i = 1,325 do\n",
    "   testData.data[i] = image.rgb2yuv(testData.data[i])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize each feature (channel) globally\t\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Name channels for convenience\n",
    "channels = {'y','u','v'}\n",
    "\n",
    "-- Normalize each channel, and store mean/std\n",
    "-- per channel. These values are important, as they are part of\n",
    "-- the trainable parameters. At test time, test data will be normalized\n",
    "-- using these values.\n",
    "print '==> preprocessing data: normalize each feature (channel) globally'\n",
    "mean = {}\n",
    "std = {}\n",
    "for i,channel in ipairs(channels) do\n",
    "   -- normalize each channel globally:\n",
    "   mean[i] = trainData.data[{ {},i,{},{} }]:mean()\n",
    "   std[i] = trainData.data[{ {},i,{},{} }]:std()\n",
    "   trainData.data[{ {},i,{},{} }]:add(-mean[i])\n",
    "   trainData.data[{ {},i,{},{} }]:div(std[i])\n",
    "end\n",
    "\n",
    "-- Normalize test data, using the training means/stds\n",
    "for i,channel in ipairs(channels) do\n",
    "   -- normalize each channel globally:\n",
    "   testData.data[{ {},i,{},{} }]:add(-mean[i])\n",
    "   testData.data[{ {},i,{},{} }]:div(std[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> preprocessing data: normalize all three channels locally\t\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Local normalization\n",
    "print '==> preprocessing data: normalize all three channels locally'\n",
    "\n",
    "-- Define the normalization neighborhood:\n",
    "neighborhood = image.gaussian1D(13)\n",
    "\n",
    "-- Define our local normalization operator (It is an actual nn module, \n",
    "-- which could be inserted into a trainable model):\n",
    "normalization = nn.SpatialContrastiveNormalization(1, neighborhood)\n",
    "\n",
    "-- Normalize all channels locally:\n",
    "for c in ipairs(channels) do\n",
    "   for i = 1,1685 do\n",
    "      trainData.data[{ i,{c},{},{} }] = normalization:forward(trainData.data[{ i,{c},{},{} }])\n",
    "   end\n",
    "   for i = 1,325 do\n",
    "      testData.data[{ i,{c},{},{} }] = normalization:forward(testData.data[{ i,{c},{},{} }])\n",
    "   end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification de la normalisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> verify statistics\t\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "training data, y-channel, mean: -0.014013743499958\t\n",
       "training data, y-channel, standard deviation: 0.86405351043886\t\n",
       "test data, y-channel, mean: -0.017142687245374\t\n",
       "test data, y-channel, standard deviation: 0.86482575390219\t\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "training data, u-channel, mean: 0.01728598555043\t\n",
       "training data, u-channel, standard deviation: 0.86037305276243\t\n",
       "test data, u-channel, mean: 0.0192645977838\t\n",
       "test data, u-channel, standard deviation: 0.86349895101673\t\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "training data, v-channel, mean: -0.015199994790522\t\n",
       "training data, v-channel, standard deviation: 0.85595064563064\t\n",
       "test data, v-channel, mean: -0.017566817668869\t\n",
       "test data, v-channel, standard deviation: 0.8590854394834\t\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print '==> verify statistics'\n",
    "for i,channel in ipairs(channels) do\n",
    "   trainMean = trainData.data[{ {},i }]:mean()\n",
    "   trainStd = trainData.data[{ {},i }]:std()\n",
    "\n",
    "   testMean = testData.data[{ {},i }]:mean()\n",
    "   testStd = testData.data[{ {},i }]:std()\n",
    "\n",
    "   print('training data, '..channel..'-channel, mean: ' .. trainMean)\n",
    "   print('training data, '..channel..'-channel, standard deviation: ' .. trainStd)\n",
    "\n",
    "   print('test data, '..channel..'-channel, mean: ' .. testMean)\n",
    "   print('test data, '..channel..'-channel, standard deviation: ' .. testStd)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 64,
       "width": 64
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 64,
       "width": 64
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Console does not support images"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 64,
       "width": 64
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first256Samples_y = trainData.data[{ 1,1 }]\n",
    "   first256Samples_u = trainData.data[{ 1,2 }]\n",
    "   first256Samples_v = trainData.data[{ 1,3 }]\n",
    "   itorch.image(first256Samples_y)\n",
    "   itorch.image(first256Samples_u)\n",
    "   itorch.image(first256Samples_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Normalisation différente de celle du fichier test. Pourquoi ? Quelle différence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- define model to train\n",
    "model = nn.Sequential()\n",
    "\n",
    "--[[ stage 1 : mean+std normalization -> filter bank -> squashing -> max pooling\n",
    "-- Convolution\n",
    "model:add(nn.SpatialConvolutionMM(3,32,5,5))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(2, 2, 2, 2))\n",
    "\n",
    "-- stage 2 : filter bank -> squashing -> max pooling\n",
    "model:add(nn.SpatialConvolutionMM(32,32,5,5))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(2, 2, 2, 2))\n",
    "]]\n",
    "\n",
    "-- stage 3 : standard 2-layer neural network\n",
    "-- Neural Network Linéaire\n",
    "model:add(nn.View(3*64*64))\n",
    "model:add(nn.Linear(3*64*64, #classes))\n",
    "--model:add(nn.ReLU())\n",
    "--model:add(nn.Linear(128,#classes))\n",
    "model:add(nn.LogSoftMax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '==> vizualisation du modèle'\n",
    "print(model:__tostring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- retrieve parameters and gradients. this helps us to use the optim package\n",
    "parameters,gradParameters = model:getParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.ClassNLLCriterion\n",
       "{\n",
       "  sizeAverage : true\n",
       "  output : 0\n",
       "  gradInput : DoubleTensor - empty\n",
       "  output_tensor : DoubleTensor - size: 1\n",
       "  target : LongTensor - size: 1\n",
       "  total_weight_tensor : DoubleTensor - size: 1\n",
       "}\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- loss function: negative log-likelihood\n",
    "criterion = nn.ClassNLLCriterion()\n",
    "print (criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 64 -- sets the mini-Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- this matrix records the current confusion across classes\n",
    "confusion = optim.ConfusionMatrix(classes)\n",
    "\n",
    "--Verfication qu'ils aient tous des labels\n",
    "temp=0\n",
    "for i=1,1685 do \n",
    "    if trainData.labels[i] == 0 then\n",
    "        temp= temp+1\n",
    "        print(i)\n",
    "    end\n",
    "end\n",
    "print (temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- training function\n",
    "function train(dataset)\n",
    "   -- epoch tracker\n",
    "   epoch = epoch or 1\n",
    "\n",
    "   -- do one epoch\n",
    "   print('<trainer> on training set:')\n",
    "   print(\"<trainer> online epoch # \" .. epoch .. ' [batchSize = ' .. batchSize .. ']')\n",
    "   for t = 1,1683,batchSize do\n",
    "\n",
    "      -- create mini batch\n",
    "      local inputs = {}\n",
    "      local targets = {}\n",
    "      for i = t,math.min(t+batchSize-1,1683) do\n",
    "         -- load new sample\n",
    "         local input = dataset.data[i]\n",
    "         local target = dataset.labels[i]\n",
    "         table.insert(inputs, input)\n",
    "         table.insert(targets, target)\n",
    "      end\n",
    "      -- create closure to evaluate f(X) and df/dX\n",
    "      local feval = function(x)\n",
    "                       -- get new parameters\n",
    "                       if x ~= parameters then\n",
    "                          parameters:copy(x)\n",
    "                       end\n",
    "\n",
    "                       -- reset gradients\n",
    "                       gradParameters:zero()\n",
    "\n",
    "                       -- f is the average of all criterions\n",
    "                       local f = 0\n",
    "\n",
    "                       -- evaluate function for complete mini batch\n",
    "                       for i = 1,#inputs do\n",
    "                          -- estimate f\n",
    "                          local output = model:forward(inputs[i])\n",
    "                          local err = criterion:forward(output, targets[i])\n",
    "                          f = f + err\n",
    "\n",
    "                          -- estimate df/dW\n",
    "                          local df_do = criterion:backward(output, targets[i])\n",
    "                          model:backward(inputs[i], df_do)\n",
    "\n",
    "                          -- update confusion\n",
    "                          confusion:add(output, targets[i])                        \n",
    "                       end\n",
    "\n",
    "                       -- normalize gradients and f(X)\n",
    "                       gradParameters:div(#inputs)\n",
    "                       f = f/#inputs\n",
    "\n",
    "                       -- return f and df/dX\n",
    "                       return f,gradParameters\n",
    "                    end\n",
    "\n",
    "\n",
    "      config = config or {learningRate = 1e-3,\n",
    "              weightDecay = 0,\n",
    "                momentum = 0,\n",
    "              learningRateDecay = 5e-7}\n",
    "      optim.sgd(feval, parameters, config)\n",
    "   end\n",
    "\n",
    "   -- print confusion matrix\n",
    "   print(confusion)\n",
    "   confusion:zero()\n",
    "\n",
    "   -- next epoch\n",
    "   epoch = epoch + 1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trainer> on training set:\t\n",
       "<trainer> online epoch # 1 [batchSize = 64]\t\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "...s/alexandreattia/torch/install/share/lua/5.1/nn/THNN.lua:109: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/alexandreattia/torch/extra/nn/lib/THNN/generic/ClassNLLCriterion.c:38\nstack traceback:\n\t[C]: in function 'v'\n\t...s/alexandreattia/torch/install/share/lua/5.1/nn/THNN.lua:109: in function 'ClassNLLCriterion_updateOutput'\n\t...tia/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'forward'\n\t[string \"-- training function...\"]:38: in function 'opfunc'\n\t...alexandreattia/torch/install/share/lua/5.1/optim/sgd.lua:44: in function 'sgd'\n\t[string \"-- training function...\"]:62: in function 'train'\n\t[string \"i=0...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:209: in function <...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:173>\n\t...exandreattia/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:381: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102d41bb0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "...s/alexandreattia/torch/install/share/lua/5.1/nn/THNN.lua:109: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/alexandreattia/torch/extra/nn/lib/THNN/generic/ClassNLLCriterion.c:38\nstack traceback:\n\t[C]: in function 'v'\n\t...s/alexandreattia/torch/install/share/lua/5.1/nn/THNN.lua:109: in function 'ClassNLLCriterion_updateOutput'\n\t...tia/torch/install/share/lua/5.1/nn/ClassNLLCriterion.lua:41: in function 'forward'\n\t[string \"-- training function...\"]:38: in function 'opfunc'\n\t...alexandreattia/torch/install/share/lua/5.1/optim/sgd.lua:44: in function 'sgd'\n\t[string \"-- training function...\"]:62: in function 'train'\n\t[string \"i=0...\"]:3: in main chunk\n\t[C]: in function 'xpcall'\n\t...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:209: in function <...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:173>\n\t...exandreattia/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...ndreattia/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t...exandreattia/torch/install/share/lua/5.1/itorch/main.lua:381: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0102d41bb0"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i<10 do\n",
    "    train(trainData)\n",
    "    i=i+1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- test function\n",
    "function test(dataset)\n",
    "\n",
    "   -- test over given dataset\n",
    "   print('<trainer> on testing Set:')\n",
    "   for t = 1,325 do\n",
    "      -- get new sample\n",
    "      local input = dataset.data[t]\n",
    "      local target = dataset.labels[t]\n",
    "\n",
    "      -- test sample\n",
    "      local pred = model:forward(input)\n",
    "      confusion:add(pred, target)\n",
    "   end\n",
    "\n",
    "   -- print confusion matrix\n",
    "   print(confusion)\n",
    "   confusion:zero()\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
